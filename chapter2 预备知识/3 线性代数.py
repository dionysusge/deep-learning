import torch
from PIL.ImagePalette import sepia

# 标量 --只含有一个数值，变量表示未知的标量值
x = torch.tensor([3.0])
y = torch.tensor([2.0])
print(x + y, x * y, x / y, x**y)

# 向量，可以看做一个标量值组成的列表，这些标量值为向量的元素或者分量
x = torch.arange(4)
print(x)

# 可以通过索引来访问任一元素
print(x[3])

# 可以用len()函数访问向量的长度
print(len(x))
# 可以用shape属性访问向量的形状
print(x.shape)

# 矩阵，矩阵可以看做是向量的推广，矩阵是一个二维数组，每个元素都是一个向量
A = torch.arange(20).reshape(5, 4)
print(A)
# 矩阵的转置，将矩阵的行和列交换
print(A.T)
# 对称矩阵，对称矩阵是一个矩阵，它的转置等于它本身
B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
print(B == B.T)

# 张量，用于构建具有更多轴的数据结构
# 图像以n维数组的形式存储，其中三个轴分别对应高度、宽度和通道，通道数为3表示RGB图像，通道数为1表示灰度图像
# 视频以n+1维数组的形式存储，其中第一个轴对应时间轴，其他轴对应空间轴
x = torch.arange(24).reshape(2, 3, 4)
print(x)

# 张量算法的基本性质，有条基本的是，但尺寸不一致时，会触发广播机制，即小尺寸的张量会被复制，直到和大尺寸的张量的尺寸相同，然后进行运算
B = A.clone()
print("A张量如下：", "-" * 50, A,"下面是A的运算", "-" * 50, A + B, sep="\n")
#
print("-" * 50,"张量乘法", A * B, sep="\n")

# 高维张量的运算
A = torch.arange(24).reshape(2, 3, 4)  # 形状为 (2, 3, 4)
B = torch.arange(40).reshape(2, 4, 5)  # 形状为 (2, 4, 5)

# 批量矩阵乘法，除最后两个维度外的的尺寸必须是相同的，最后两个维度进行矩阵乘法，可以不同，但要满足矩阵乘法要求
C = torch.matmul(A, B)  # 形状为 (2, 3, 5)
print("批量矩阵乘法结果形状:", C.shape, "-"*50, sep="\n")

# 与标量的运算，相当于每个元素都和该标量运算
print(A * 2)

# 降维
print("-" * 50, "降维操作", "A张量如下", A, sep="\n" )
# 求和，对所有元素求和，返回一个标量
print("求和", A.sum())

# 指定轴进行求和，使该轴在输出中消失
print("沿着第一个轴降维，把两个块加起来，合成一块", A.sum(axis=0), sep="\n") # 对第0轴进行求和，即对第0轴的元素求和，返回一个形状为(3, 4)的张量
print("沿着第二个轴降维，把两个块里的行各自加起来，合成一行", A.sum(axis=1), sep="\n") # 对第1轴进行求和，即对第1轴的元素求和，返回一个形状为(2, 4)的张量

# 沿着行列求和，返回一个标量
print("沿着第一、二轴降维，块加起来、行加起来，合成一个向量，对应有几列", A.sum(axis=[0, 1]), sep="\n") # 对第0轴进行求和，即对第0轴的元素求和，返回一个形状为(3, 4)的张量

# 计算平均值，这里要将A转化为float类型才可以
A = A.float()
print("计算A张量所有元素平均值", A.mean(), sep="\n") # 对所有元素求平均值，返回一个标量
# 计算平均值，指定轴进行求和，使该轴在输出中消失
print("沿着第一个轴降维，把两个块相同位置的元素求平均值，合成一块", A.mean(axis=0), sep="\n") # 对第0轴进行求和，即对第0轴的元素求和，返回一个形状为(3, 4)的张量

# 非降维求和，可以看到在输出里面，求和的轴仍然存在，只是元素个数为1，比如下面，有一行
sum_A = A.sum(axis=1, keepdims=True) # 对第1轴进行求和，即对第1轴的元素求和，返回一个形状为(2, 1)的张量
print("非降维求和", sum_A, sep="\n") # 对所有元素求平均值，返回一个标量

# 可以通过广播机制，将sum_A和A进行相除，那么就实现了某一维度的归一化，兄弟
print("广播机制，对行归一化处理", A / sum_A, sep="\n") # 对所有元素求平均值，返回一个标量

# 沿着某个轴计算累加和，计算结果形状不变，保留到该轴的最后一个元素里面
print(A.cumsum(axis=0))

# 点积、矩阵乘法，这里注意它和直接用*区别，*是直接相同位置元素相乘，而不是矩阵乘法
B = torch.ones((2,4,5),dtype=torch.float32)
# # 这里mm函数是专门处理矩阵乘法的，对三维张量会报错
# print(torch.mm(A, B))
# 适应不同尺寸的张量，应该用matmul函数
print(torch.matmul(A, B))

# 创建一个 3x4 的矩阵
A = torch.arange(12).reshape(3, 4)
# 创建一个 4x5 的矩阵
B = torch.arange(20).reshape(4, 5)

# 执行矩阵乘法
C = torch.matmul(A, B)

print("矩阵 A:")
print(A)
print("矩阵 B:")
print(B)
print("矩阵 A 和 B 相乘的结果 C:")
print(C)
print("结果矩阵 C 的形状:", C.shape)

# 范数，类似于距离，每个元素的平方总和再去开根
print("-" * 50)
u = torch.rand(2, 3, 4)
print(u)
print(torch.norm(u))