# 解决分类问题
# 同样的是一个线性模型，只是输出不再是单一的，映射到多个输出
# 使用softmax方法，取指数再除以指数和，得到非负且0到1的数据，便是概率

# 损失函数为 -y * log(y_hat)，称为交叉熵损失，对数里的值恒为负，最后的损失恒为正。并且y是一个独热编码向量，相当于做了一次选择
# 不过这里我有个问题，如果出现异常值较大的情况，由于对数是非线性的，训练过程中就算每步学习率调得比较小，也会出现意外的结果来影响参数准确率

# 要最小化损失函数，还是采用梯度下降的方法，参数朝着梯度的反方向移动（这里突然意识到，“-”只是表示方向，不一定真是让参数减小，因为梯度可能是负的呀）

# 求梯度，发现其形式相当简洁，恰为softmax(o_j) - y_j

# 分类问题和预测不一样，训练后的参数应用到真实场景（测试集）上时只能判断是或否，由此引出“准确率”的概念